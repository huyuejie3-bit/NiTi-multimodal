# Guide to Using the Superelastic Degradation Prediction Model

## 1. Project Overview

This project is a multi-modal fusion model based on multi-channel separation for predicting the superelastic degradation of NiTi alloys.

The model combines material microstructure and loading conditions for prediction. The project has been organized into the following directory structure according to research-grade standards:

```
project/
├── analysis/        # Analysis results and code

│ ├── shap/         # SHAP interpretive analysis results

│ └── visualization/        # Other visualization analysis results

├── checkpoints/        # Model checkpoint saving

├── data/        # Raw data and processed data

├── research/         # Research-related documents

│ ├── figures/         # Paper-level figures and visualizations

│ └── papers/         # Related papers and references

├── scripts/         # Script files

│ ├── analysis/         # Analysis scripts

│ ├── evaluation/         # Evaluation scripts

│ └── utils/         # Utility scripts

├── src/         # Source code

└── tensorboard_logs/         # TensorBoard logs

```

## 2. Environment Configuration

Before use, please ensure all dependencies are installed:

```bash pip install -r requirements.txt

```
The main required dependencies include:

- PyTorch 1.7+

- TensorFlow 2.0+ (for TensorBoard)

- NumPy

- Matplotlib

- scikit-learn

- SHAP

- OpenCV

## 3. Main Scripts and Functions

### 3.1 Training and Testing the Model (main.py)

The main program entry point, used for training, testing, or using the model for prediction.

#### Basic Usage:

**Training the Model:**

```bash
python main.py --mode train --epochs 150 --batch_size 32

```

**Testing the Model:**

```bash
python main.py --mode test --checkpoint ./checkpoints/best_model.pth

```

**Using the Model for Prediction:**

```bash
python main.py --mode predict --checkpoint ./checkpoints/best_model.pth

```

#### Important Parameter Explanation:

- `--mode`: Running mode, options are `train`, `test`, or `predict`

- `--data_dir`: Data directory, default is `./data`

- `--checkpoint`: Model checkpoint path

- `--save_dir`: Save directory, default is `./checkpoints/timestamps`

- `--epochs`: Number of training epochs, default is 150

- `--batch_size`: Batch size, default is 32

- `--lr`: Learning rate, default is 0.001

- `--seed`: Random seed, default is 42

- `--device`: Device to use (CUDA or CPU), automatically selected by default


#### Modifying the number of inputs and outputs:


- `--num_images`: Number of input images, defaults to 3 in the configuration file

- `--num_params`: Number of input numerical parameters, defaults to 3 in the configuration file

- `--num_outputs`: Number of output performance metrics, defaults to 1 in the configuration file


These parameters can also be permanently changed by modifying the `DATA_CONFIG` dictionary in the `src/config.py` file.

### 3.2 SHAP Analysis (scripts/analysis/analyze_shap.py)

The SHAP method is used to analyze the model's decision-making process and interpret the importance of features.

#### Usage:

```bash python scripts/analysis/analyze_shap.py --checkpoint ./checkpoints/best_model.pth --num_samples 980 --batch_size 4

```

#### Parameter Explanation:

- `--checkpoint`: Path to model checkpoints

- `--num_samples`: Number of samples used for analysis, default is 980

- `--batch_size`: Batch size, default is 4 (smaller values ​​reduce memory usage)

This script generates various SHAP analysis plots, saved in the `analysis/shap` and `research/figures` directories:

1. Feature importance summary plot (shap_values_all.png)

2. Modal importance pie chart (modal_importance_pie.png)

3. Dependency plot of the most important feature (shap_dependence_FeatureName.png)

### 3.3 Custom SHAP Visualization (scripts/analysis/create_custom_shap_plot.py)

Generates optimized SHAP visualizations with English labels and more data points.

#### Usage:

```bash
python scripts/analysis/create_custom_shap_plot.py

```

### 3.4 Model Visualization (visualize_model.py)

Used to generate visual representations of model structures for easier understanding of model architecture.

#### Usage:

```bash
python visualize_model.py

```
This script generates various model visualizations:

1. Model computation graph generated by torchviz

2. TensorBoard model graph

3. Model hierarchy diagram

4. Model processing flowchart

5. Detailed text structure description

### 3.5 Model Testing (test_model.py)

A simple script for testing model initialization and forward propagation functionality.

#### Usage:

```bash
python test_model.py

```

This script tests single-image and multi-image input scenarios and outputs the key structural dimensions of the model.

## 4. Configuration Modification

### 4.1 Modifying Data Configuration

Data configuration is in the `DATA_CONFIG` dictionary in the `src/config.py` file:

```python

DATA_CONFIG = {

'num_images': 3, # Number of input images

'image_size': 336, # Image size

'num_parameters': 3, # Number of numerical parameters

'num_outputs': 1, # Number of output metrics

'dataset_split': [0.7, 0.15, 0.15] # Training/validation/test set split ratio

}
```

To modify the number of input images or parameters, simply edit the corresponding values. These changes affect the model architecture, thus requiring model retraining.

### 4.2 Modifying Model Configuration

Model configuration is in the `MODEL_CONFIG` dictionary of the `src/config.py` file:

# Model configuration
MODEL_CONFIG = {

'backbone': 'resnet18', # CNN backbone network to use, options: 'resnet18', 'resnet34', 'resnet50'

'pretrained': True, # Whether to use pretrained weights

'feature_dim': 512, # Feature dimension

'dropout': 0.5, # Dropout ratio

'use_attention': True, # Whether to use attention mechanism, can use Transformer

}

These parameters can be modified to adjust the model architecture. For example:

- Use other backbones ('resnet34', 'resnet50', etc.)

- Increase or decrease the number of transformer layers

- Adjust the number of attention heads

- Change the dropout ratio to control overfitting

### 4.3 Modifying the Training Configuration

The training configuration is in the `TRAIN_CONFIG` dictionary in the `src/config.py` file:

``python
TRAIN_CONFIG = {

'batch_size': 32,

'num_epochs': 1,

'learning_rate': 1e-3,

'weight_decay': 1e-4,

'lr_scheduler': 'cosine', # Learning rate scheduler, options: 'step', 'cosine', 'plateau'

'early_stopping': 10, # Early stopping patience

'save_dir': './checkpoints',

}
```

These parameters can be adjusted as needed to optimize the training process.

## 5. TensorBoard Visualization

Various metrics are automatically recorded during training, which can be viewed using TensorBoard:

```bash python -m tensorboard.main --logdir=tensorboard_logs/runs --port=6006

```

Then, access http://localhost:6006 in your browser to view the training history, model graphs, and generated visualizations.

## 6. Input Data Format

### Image Data

- Images should be placed in the `data` directory.

- File naming format: `sample_i_j.jpg`, where i is the sample index and j is the image index.

- For example, for sample 0, if there are 3 images, the corresponding file names would be `sample_0_0.jpg`, `sample_0_1.jpg`, and `sample_0_2.jpg`.

### Numerical Parameters and Target Values

- Numerical parameters should be saved as a NumPy file `data/parameters.npy`.

- Target values ​​should be saved as a NumPy file `data/targets.npy`.

- The parameter array shape should be (n_samples, n_parameters).

- The target array shape should be (n_samples, n_outputs).

## 7. Common Problems and Solutions

### Insufficient Memory Issues

Insufficient memory issues may occur during SHAP analysis. This can be resolved by:

- Reducing memory usage... `batch_size` parameter value

- Decrease the `num_samples` parameter value

- For particularly large models, a CUDA-enabled GPU may be required.

### Port Concurrency Issue

When starting TensorBoard, you may encounter a port conflict error:

```
ERROR: TensorBoard could not bind to port 6006, it was already in use

```
Solution: Use a different port number

```bash
python -m tensorboard.main --logdir=tensorboard_logs/runs --port=6007

```
### Image Feature Extraction Issue

If SHAP analysis shows that image features contribute almost nothing to the prediction results, you may need to:

1. Check the image data quality

2. Modify the CNN backbone network (change 'backbone' in MODEL_CONFIG)

3. Redesign the feature extraction part (modify src/models/cnn_model.py)

## 8. Advanced Customization

### Custom Model Structure

For in-depth modifications to the model structure, edit the `src/models/cnn_model.py` file. Main classes include:

- `MaterialCNN`: The main model class

- `ConvBlock`, `ResidualBlock`, `SEBlock`, etc.: Various modules

### Adding New Analysis Methods

To add new analysis methods, it is recommended to create new scripts in the `scripts/analysis/` directory and organize the code according to the existing SHAP analysis script structure.

### Custom Datasets

To use custom datasets, modify the `MaterialDataset` class in `src/data/dataset.py`.

### Visualizing Heatmaps of Specific Images (New Feature)

Command: python scripts/analysis/generate_heatmap.py --checkpoint checkpoints/model_directory/best_model.pth --sample sample_index --image image_index

Parameter Description:

- --checkpoint: Path to model checkpoints, e.g., checkpoints/20250330_130512/best_model.pth

- --sample: Index of the sample to analyze (integer), e.g., 0 represents the first sample

- --image: Image index in the sample (integer), e.g., 0, 1, 2, etc. (If not specified, heatmaps will be generated for all images)

- --layer: Target layer name (optional), used for Grad-CAM analysis; if not specified, the last convolutional layer is used

- --alpha: Heatmap overlay transparency (between 0 and 1), default is 0.5

- --output_dir: Directory to save results (optional), default is analysis/visualization

Example:

- Generate a heatmap for image 1 of sample 0:

python `scripts/analysis/generate_heatmap.py --checkpoint checkpoints/20250330_130512/best_model.pth --sample 0 --image 1`

- Generate heatmaps for all images of Sample 2:

`python scripts/analysis/generate_heatmap.py --checkpoint checkpoints/20250330_130512/best_model.pth --sample 2`

- Use a custom output directory:

`python scripts/analysis/generate_heatmap.py --checkpoint checkpoints/20250330_130512/best_model.pth --sample 1 --image 0 --output_dir my_heatmaps`

Note: This function generates heatmap visualizations for specified samples and images, helping you understand which areas of the material images the model focuses on.

The heatmap will be saved in the specified directory (default is analysis/visualization) and automatically copied to the research/figures and tensorboard_logs/images directories.

The generated heatmap includes the original image, the activation heatmap, and the overlay effect, with filenames in the format "heatmap_sample{sample index}_image{image index}.png".

---

For any questions, please refer to the project's README.md file or the documentation strings for each script.